import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
import random
import numpy as np
import json
from pathlib import Path
from datasets import Dataset
from torch.utils.data import DataLoader

from config import config
from transformer import create_model, count_parameters
from data_utils import Lang, tokenize, encode_and_pad

def set_seed(seed):
    """ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def load_translation_data(data_dir, train_filename, val_filename):
    """ë²ˆì—­ ë°ì´í„° ë¡œë“œ (main.py ë‚´ë¶€ìš©)"""
    data_dir = Path(data_dir)
    
    train_file = data_dir / f"{train_filename}.json"
    val_file = data_dir / f"{val_filename}.json"
    
    if not train_file.exists() or not val_file.exists():
        raise FileNotFoundError(
            f"í•„ìš”í•œ íŒŒì¼:\n- {train_file}\n- {val_file}"
        )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {train_file}")
    print(f"ê²€ì¦ ë°ì´í„°: {val_file}")
    
    # JSON íŒŒì¼ ë¡œë“œ
    with open(train_file, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
    with open(val_file, 'r', encoding='utf-8') as f:
        val_data = json.load(f)
    
    # data í•„ë“œ ì¶”ì¶œ
    if isinstance(train_data, dict) and 'data' in train_data:
        train_data = train_data['data']
    if isinstance(val_data, dict) and 'data' in val_data:
        val_data = val_data['data']
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
    train_dict = {
        'korean': [item['ko'] for item in train_data],
        'english': [item['en'] for item in train_data]
    }
    val_dict = {
        'korean': [item['ko'] for item in val_data],
        'english': [item['en'] for item in val_data]
    }
    
    # datasets.Datasetìœ¼ë¡œ ë³€í™˜
    train_dataset = Dataset.from_dict(train_dict)
    val_dataset = Dataset.from_dict(val_dict)
    
    return train_dataset, val_dataset

def build_vocab(dataset, field, min_freq):
    """ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•"""
    lang = Lang(name=field, min_freq=min_freq)
    for ex in dataset:
        tokens = tokenize(ex[field])
        if tokens:
            lang.add_word(tokens)
    lang.build()
    return lang

def get_max_length(dataset, field):
    """ìµœëŒ€ ê¸¸ì´ ê³„ì‚°"""
    lengths = [len(tokenize(ex[field])) for ex in dataset]
    return max(lengths) + 2  # SOS, EOS ê³ ë ¤

class TranslationDataset:
    """ê°„ë‹¨í•œ ë²ˆì—­ ë°ì´í„°ì…‹"""
    def __init__(self, dataset, src_lang, tgt_lang, src_max_len, tgt_max_len):
        self.dataset = dataset
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang
        self.src_max_len = src_max_len
        self.tgt_max_len = tgt_max_len

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, i):
        ex = self.dataset[i]
        
        # ì†ŒìŠ¤ ì²˜ë¦¬
        src_tokens = tokenize(ex['korean'])
        if not src_tokens:
            src_tokens = ['<UNK>']
        src_ids = encode_and_pad(src_tokens, self.src_lang, self.src_max_len)
        
        # íƒ€ê²Ÿ ì²˜ë¦¬
        tgt_tokens = tokenize(ex['english'])
        if not tgt_tokens:
            tgt_tokens = ['<UNK>']
        
        # <SOS> + tokens + <EOS> í˜•íƒœ
        tgt_tokens = ['<SOS>'] + tgt_tokens[:self.tgt_max_len - 2] + ['<EOS>']
        tgt_ids = [self.tgt_lang.word2index.get(t, self.tgt_lang.word2index['<UNK>']) for t in tgt_tokens]
        tgt_ids += [self.tgt_lang.word2index['<PAD>']] * (self.tgt_max_len - len(tgt_ids))
        tgt_ids = tgt_ids[:self.tgt_max_len]
        
        return {
            'src_ids': torch.tensor(src_ids, dtype=torch.long),
            'tgt_ids': torch.tensor(tgt_ids, dtype=torch.long)
        }

def train_epoch(model, train_loader, optimizer, criterion, device):
    """í•œ ì—í¬í¬ í›ˆë ¨"""
    model.train()
    total_loss = 0
    num_batches = len(train_loader)
    
    progress_bar = tqdm(train_loader, desc="Training", leave=False)
    
    for batch_idx, batch in enumerate(progress_bar):
        src_ids = batch['src_ids'].to(device)
        tgt_ids = batch['tgt_ids'].to(device)
        
        # Teacher forcing
        tgt_input = tgt_ids[:, :-1]
        tgt_output = tgt_ids[:, 1:]
        
        # Forward pass
        optimizer.zero_grad()
        logits = model(src_ids, tgt_input)
        
        # Loss ê³„ì‚°
        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))
        
        # Backward pass
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.MAX_GRAD_NORM)
        optimizer.step()
        
        total_loss += loss.item()
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'avg_loss': f'{total_loss / (batch_idx + 1):.4f}'
        })
    
    return total_loss / num_batches

def evaluate(model, val_loader, criterion, device):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        progress_bar = tqdm(val_loader, desc="Evaluating", leave=False)
        
        for batch in progress_bar:
            src_ids = batch['src_ids'].to(device)
            tgt_ids = batch['tgt_ids'].to(device)
            
            tgt_input = tgt_ids[:, :-1]
            tgt_output = tgt_ids[:, 1:]
            
            logits = model(src_ids, tgt_input)
            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))
            
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return total_loss / len(val_loader)

def translate_sample(model, src_text, src_lang, tgt_lang, device):
    """ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸"""
    model.eval()
    
    try:
        with torch.no_grad():
            # ì†ŒìŠ¤ ë¬¸ì¥ ì²˜ë¦¬
            src_tokens = tokenize(src_text)
            if not src_tokens:
                src_tokens = ['<UNK>']
            
            src_ids = encode_and_pad(src_tokens, src_lang, 128)
            src_tensor = torch.tensor([src_ids], device=device)
            
            # ìƒì„±
            generated = model.generate(
                src_tensor, 
                max_length=config.GENERATION_MAX_LENGTH,
                start_token=tgt_lang.word2index['<SOS>'], 
                end_token=tgt_lang.word2index['<EOS>']
            )
            
            # ë””ì½”ë”©
            generated_ids = generated[0].cpu().tolist()
            translated_tokens = []
            for id in generated_ids:
                if id == tgt_lang.word2index['<EOS>']:
                    break
                if id not in [tgt_lang.word2index['<PAD>'], tgt_lang.word2index['<SOS>']]:
                    word = tgt_lang.index2word.get(id, '<UNK>')
                    translated_tokens.append(word)
            
            return ' '.join(translated_tokens)
    
    except Exception as e:
        return f"[ë²ˆì—­ ì‹¤íŒ¨: {str(e)}]"

def plot_training_progress(train_losses, val_losses, save_path=None):
    """í›ˆë ¨ ì§„í–‰ ìƒí™© ê·¸ë˜í”„"""
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Progress')
    plt.legend()
    plt.grid(True)
    
    if save_path:
        plt.savefig(save_path)
        print(f"ğŸ“Š ê·¸ë˜í”„ ì €ì¥: {save_path}")
    
    plt.show()

def run_translation_tests(model, src_lang, tgt_lang, device):
    """ë²ˆì—­ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    test_sentences = [
        "ì•ˆë…•í•˜ì„¸ìš”.",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤.",
        "ì €ëŠ” í•™ìƒì…ë‹ˆë‹¤.",
        "ê°ì‚¬í•©ë‹ˆë‹¤.",
        "ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”."
    ]
    
    for sentence in test_sentences:
        translated = translate_sample(model, sentence, src_lang, tgt_lang, device)
        print(f"í•œêµ­ì–´: {sentence}")
        print(f"ì˜ì–´: {translated}")
        print("-" * 40)

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    # ì„¤ì • ì¶œë ¥
    config.print_config()
    
    # ì‹œë“œ ì„¤ì •
    set_seed(config.RANDOM_SEED)
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = Path(config.SAVE_DIR)
    save_dir.mkdir(exist_ok=True)
    
    print(f"\nğŸš€ Transformer ë²ˆì—­ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print(f"ë””ë°”ì´ìŠ¤: {config.DEVICE}")
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“š ë°ì´í„° ë¡œë”© ì¤‘...")
    try:
        train_dataset, val_dataset = load_translation_data(
            config.DATA_DIR, config.TRAIN_FILENAME, config.VALID_FILENAME
        )
        
        print(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: {len(train_dataset):,}")
        print(f"ê²€ì¦ ë°ì´í„° í¬ê¸°: {len(val_dataset):,}")
        
        # ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•
        print("ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• ì¤‘...")
        src_lang = build_vocab(train_dataset, 'korean', config.MIN_FREQ)
        tgt_lang = build_vocab(train_dataset, 'english', config.MIN_FREQ)
        
        print(f"ì†ŒìŠ¤ ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {src_lang.n_words:,}")
        print(f"íƒ€ê²Ÿ ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {tgt_lang.n_words:,}")
        
        # ìµœëŒ€ ê¸¸ì´ ê³„ì‚°
        src_max_len = get_max_length(train_dataset, 'korean')
        tgt_max_len = get_max_length(train_dataset, 'english')
        
        # ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„±
        train_ds = TranslationDataset(train_dataset, src_lang, tgt_lang, src_max_len, tgt_max_len)
        val_ds = TranslationDataset(val_dataset, src_lang, tgt_lang, src_max_len, tgt_max_len)
        
        train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False)
        
    except FileNotFoundError as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        return
    
    # ëª¨ë¸ ìƒì„±
    print("\nğŸ—ï¸ ëª¨ë¸ ìƒì„± ì¤‘...")
    model = create_model(
        src_vocab_size=src_lang.n_words,
        tgt_vocab_size=tgt_lang.n_words,
        d_model=config.D_MODEL,
        num_heads=config.NUM_HEADS,
        num_layers=config.NUM_LAYERS,
        ff_size=config.FF_SIZE,
        dropout=config.DROPOUT,
        pad_token_id=src_lang.word2index['<PAD>']
    )
    
    model = model.to(config.DEVICE)
    print(f"ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ìˆ˜: {count_parameters(model):,}")
    
    # Loss & Optimizer
    criterion = nn.CrossEntropyLoss(ignore_index=tgt_lang.word2index['<PAD>'])
    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)
    
    # í›ˆë ¨ ë£¨í”„
    print(f"\nğŸ”¥ í›ˆë ¨ ì‹œì‘! ({config.NUM_EPOCHS} ì—í¬í¬)")
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    
    for epoch in range(config.NUM_EPOCHS):
        print(f"\nğŸ“Š Epoch {epoch + 1}/{config.NUM_EPOCHS}")
        
        # í›ˆë ¨
        train_loss = train_epoch(model, train_loader, optimizer, criterion, config.DEVICE)
        train_losses.append(train_loss)
        
        # í‰ê°€
        val_loss = evaluate(model, val_loader, criterion, config.DEVICE)
        val_losses.append(val_loss)
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_loss)
        
        print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
                'src_lang': src_lang,
                'tgt_lang': tgt_lang,
                'config': config
            }, save_dir / 'best_model.pt')
            print("ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥!")
        
        # ë²ˆì—­ í…ŒìŠ¤íŠ¸
        if (epoch + 1) % config.TRANSLATE_EVERY == 0:
            print("\nğŸ” ë²ˆì—­ í…ŒìŠ¤íŠ¸:")
            for sentence in config.TEST_SENTENCES[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                translated = translate_sample(model, sentence, src_lang, tgt_lang, config.DEVICE)
                print(f"  í•œêµ­ì–´: {sentence}")
                print(f"  ì˜ì–´: {translated}")
                print()
    
    # í›ˆë ¨ ì™„ë£Œ
    print("\n" + "=" * 60)
    print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ˆ ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {save_dir}")
    
    # ìµœì¢… ë²ˆì—­ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ìµœì¢… ë²ˆì—­ í…ŒìŠ¤íŠ¸:")
    print("=" * 60)
    run_translation_tests(model, src_lang, tgt_lang, config.DEVICE)
    
    # ìµœì¢… ê·¸ë˜í”„ ì €ì¥
    plot_path = save_dir / "training_progress.png"
    plot_training_progress(train_losses, val_losses, plot_path)
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ğŸŠ")

if __name__ == "__main__":
    main()