import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import matplotlib.pyplot as plt
import os
import random
import numpy as np
from pathlib import Path

from config import config
from transformer import create_model, count_parameters
from data_utils import make_translation_loaders_from_dir

def set_seed(seed):
    """ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def train_epoch(model, train_loader, optimizer, criterion, device):
    """í•œ ì—í¬í¬ í›ˆë ¨"""
    model.train()
    total_loss = 0
    num_batches = len(train_loader)
    
    progress_bar = tqdm(train_loader, desc="Training", leave=False)
    
    for batch_idx, batch in enumerate(progress_bar):
        src_ids = batch['src_ids'].to(device)
        tgt_ids = batch['tgt_ids'].to(device)
        
        # Teacher forcing: ì…ë ¥ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        tgt_input = tgt_ids[:, :-1]  # <sos> ~ ë§ˆì§€ë§‰ ì´ì „ê¹Œì§€
        tgt_output = tgt_ids[:, 1:]  # ì²« ë²ˆì§¸ ë‹¤ìŒ ~ <eos>ê¹Œì§€
        
        # Forward pass
        optimizer.zero_grad()
        logits = model(src_ids, tgt_input)
        
        # Loss ê³„ì‚° (íŒ¨ë”© í† í° ì œì™¸)
        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.MAX_GRAD_NORM)
        optimizer.step()
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        total_loss += loss.item()
        
        # Progress bar ì—…ë°ì´íŠ¸
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'avg_loss': f'{total_loss / (batch_idx + 1):.4f}'
        })
        
        # ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥
        if config.VERBOSE and (batch_idx + 1) % config.LOG_INTERVAL == 0:
            print(f"    Batch {batch_idx + 1}/{num_batches}, Loss: {loss.item():.4f}")
    
    return total_loss / num_batches

def evaluate(model, val_loader, criterion, device):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    num_batches = len(val_loader)
    
    with torch.no_grad():
        progress_bar = tqdm(val_loader, desc="Evaluating", leave=False)
        
        for batch in progress_bar:
            src_ids = batch['src_ids'].to(device)
            tgt_ids = batch['tgt_ids'].to(device)
            
            tgt_input = tgt_ids[:, :-1]
            tgt_output = tgt_ids[:, 1:]
            
            logits = model(src_ids, tgt_input)
            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_output.reshape(-1))
            
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return total_loss / num_batches

def translate_sample(model, src_text, src_lang, tgt_lang, device):
    """ìƒ˜í”Œ ë²ˆì—­ í…ŒìŠ¤íŠ¸"""
    from data_utils import tokenize, encode_and_pad
    
    model.eval()
    
    try:
        with torch.no_grad():
            # ì†ŒìŠ¤ ë¬¸ì¥ í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”©
            src_tokens = tokenize(src_text)
            if not src_tokens:
                src_tokens = ['<UNK>']
            
            # ì ë‹¹í•œ ê¸¸ì´ë¡œ ì¸ì½”ë”© (128ì€ ì¼ë°˜ì ì¸ ìµœëŒ€ ê¸¸ì´)
            src_ids = encode_and_pad(src_tokens, src_lang, 128)
            src_tensor = torch.tensor([src_ids], device=device)
            
            # ìƒì„±
            generated = model.generate(
                src_tensor, 
                max_length=config.GENERATION_MAX_LENGTH,
                start_token=tgt_lang.word2index['<SOS>'], 
                end_token=tgt_lang.word2index['<EOS>']
            )
            
            # ë””ì½”ë”©
            generated_ids = generated[0].cpu().tolist()
            translated_tokens = []
            for id in generated_ids:
                if id == tgt_lang.word2index['<EOS>']:
                    break
                if id not in [tgt_lang.word2index['<PAD>'], tgt_lang.word2index['<SOS>']]:
                    word = tgt_lang.index2word.get(id, '<UNK>')
                    translated_tokens.append(word)
            
            return ' '.join(translated_tokens)
    
    except Exception as e:
        return f"[ë²ˆì—­ ì‹¤íŒ¨: {str(e)}]"

def run_translation_tests(model, src_lang, tgt_lang, device):
    """ë²ˆì—­ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\nğŸ” ë²ˆì—­ í…ŒìŠ¤íŠ¸:")
    print("-" * 40)
    
    for sentence in config.TEST_SENTENCES:
        translated = translate_sample(model, sentence, src_lang, tgt_lang, device)
        print(f"  {config.SRC_FIELD}: {sentence}")
        print(f"  {config.TGT_FIELD}: {translated}")
        print()

def save_model(model, optimizer, epoch, train_loss, val_loss, src_lang, tgt_lang, filepath):
    """ëª¨ë¸ ì €ì¥"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'src_lang': src_lang,
        'tgt_lang': tgt_lang,
        'config': config
    }, filepath)

def plot_training_progress(train_losses, val_losses, save_path):
    """í›ˆë ¨ ì§„í–‰ ìƒí™© ê·¸ë˜í”„ ìƒì„±"""
    if not config.SAVE_PLOTS:
        return
    
    plt.figure(figsize=(12, 6))
    
    # Loss ê·¸ë˜í”„
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train Loss', color='blue', alpha=0.7)
    plt.plot(val_losses, label='Validation Loss', color='red', alpha=0.7)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # ìµœê·¼ ì—í¬í¬ í™•ëŒ€ ê·¸ë˜í”„
    plt.subplot(1, 2, 2)
    start_epoch = max(0, len(train_losses) - 5)  # ìµœê·¼ 5 ì—í¬í¬
    epochs = range(start_epoch, len(train_losses))
    plt.plot(epochs, train_losses[start_epoch:], label='Train Loss', color='blue', alpha=0.7, marker='o')
    plt.plot(epochs, val_losses[start_epoch:], label='Validation Loss', color='red', alpha=0.7, marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Recent Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def main():
    """ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    # ì„¤ì • ì¶œë ¥
    config.print_config()
    
    # ì‹œë“œ ì„¤ì •
    set_seed(config.RANDOM_SEED)
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = Path(config.SAVE_DIR)
    save_dir.mkdir(exist_ok=True)
    
    print(f"\nğŸš€ Transformer ë²ˆì—­ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
    print(f"ë””ë°”ì´ìŠ¤: {config.DEVICE}")
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("\nğŸ“š ë°ì´í„° ë¡œë”© ì¤‘...")
    try:
        src_lang, tgt_lang, src_max_len, tgt_max_len, train_loader, val_loader = \
            make_translation_loaders_from_dir(
                config.DATA_DIR,
                train_filename=config.TRAIN_FILENAME,
                val_filename=config.VALID_FILENAME,
                src_field=config.SRC_FIELD,
                tgt_field=config.TGT_FIELD,
                min_freq=config.MIN_FREQ,
                batch_size=config.BATCH_SIZE,
                num_workers=config.NUM_WORKERS
            )
    except FileNotFoundError as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
        print("\nğŸ“‹ í•„ìš”í•œ íŒŒì¼:")
        print(f"- {config.DATA_DIR}/{config.TRAIN_FILENAME}.json")
        print(f"- {config.DATA_DIR}/{config.VALID_FILENAME}.json")
        return
    
    print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    print(f"   ì†ŒìŠ¤ ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {src_lang.n_words:,}")
    print(f"   íƒ€ê²Ÿ ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {tgt_lang.n_words:,}")
    print(f"   í›ˆë ¨ ë°°ì¹˜ ìˆ˜: {len(train_loader):,}")
    print(f"   ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(val_loader):,}")
    
    # ëª¨ë¸ ìƒì„±
    print("\nğŸ—ï¸ ëª¨ë¸ ìƒì„± ì¤‘...")
    model = create_model(
        src_vocab_size=src_lang.n_words,
        tgt_vocab_size=tgt_lang.n_words,
        d_model=config.D_MODEL,
        num_heads=config.NUM_HEADS,
        num_layers=config.NUM_LAYERS,
        ff_size=config.FF_SIZE,
        dropout=config.DROPOUT,
        pad_token_id=src_lang.word2index['<PAD>']
    )
    
    model = model.to(config.DEVICE)
    
    total_params = count_parameters(model)
    print(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!")
    print(f"   ì´ ë§¤ê°œë³€ìˆ˜ ìˆ˜: {total_params:,}")
    print(f"   ëª¨ë¸ í¬ê¸°: ~{total_params * 4 / 1024 / 1024:.1f} MB")
    
    # Loss function & Optimizer
    criterion = nn.CrossEntropyLoss(ignore_index=tgt_lang.word2index['<PAD>'])
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY,
        betas=config.BETAS,
        eps=config.EPS
    )
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        patience=config.SCHEDULER_PATIENCE, 
        factor=config.SCHEDULER_FACTOR,
        verbose=True
    )
    
    # í›ˆë ¨ ë£¨í”„
    print(f"\nğŸ”¥ í›ˆë ¨ ì‹œì‘! ({config.NUM_EPOCHS} ì—í¬í¬)")
    print("=" * 60)
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    
    for epoch in range(config.NUM_EPOCHS):
        epoch_start_msg = f"ğŸ“Š Epoch {epoch + 1}/{config.NUM_EPOCHS}"
        print(f"\n{epoch_start_msg}")
        print("-" * len(epoch_start_msg))
        
        # í›ˆë ¨
        train_loss = train_epoch(model, train_loader, optimizer, criterion, config.DEVICE)
        train_losses.append(train_loss)
        
        # í‰ê°€
        if (epoch + 1) % config.EVAL_EVERY == 0:
            val_loss = evaluate(model, val_loader, criterion, config.DEVICE)
            val_losses.append(val_loss)
        else:
            val_loss = val_losses[-1] if val_losses else float('inf')
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ… Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.2e}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_path = save_dir / "best_model.pt"
            save_model(model, optimizer, epoch, train_loss, val_loss, src_lang, tgt_lang, best_model_path)
            print(f"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥! (Val Loss: {val_loss:.4f})")
        
        # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
        if (epoch + 1) % config.SAVE_EVERY == 0:
            checkpoint_path = save_dir / f"model_epoch_{epoch + 1}.pt"
            save_model(model, optimizer, epoch, train_loss, val_loss, src_lang, tgt_lang, checkpoint_path)
        
        # ë²ˆì—­ í…ŒìŠ¤íŠ¸
        if (epoch + 1) % config.TRANSLATE_EVERY == 0:
            run_translation_tests(model, src_lang, tgt_lang, config.DEVICE)
        
        # í›ˆë ¨ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
        if config.SAVE_PLOTS and len(val_losses) > 0:
            plot_path = save_dir / "training_progress.png"
            plot_training_progress(train_losses, val_losses, plot_path)
    
    # í›ˆë ¨ ì™„ë£Œ
    print("\n" + "=" * 60)
    print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ˆ ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {save_dir}")
    
    # ìµœì¢… ë²ˆì—­ í…ŒìŠ¤íŠ¸
    print("\nğŸ” ìµœì¢… ë²ˆì—­ í…ŒìŠ¤íŠ¸:")
    print("=" * 60)
    run_translation_tests(model, src_lang, tgt_lang, config.DEVICE)
    
    # ìµœì¢… ê·¸ë˜í”„ ì €ì¥
    if config.SAVE_PLOTS:
        final_plot_path = save_dir / "final_training_progress.png"
        plot_training_progress(train_losses, val_losses, final_plot_path)
        print(f"ğŸ“Š í›ˆë ¨ ê·¸ë˜í”„ ì €ì¥: {final_plot_path}")
    
    print(f"\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ğŸŠ")

if __name__ == "__main__":
    main()